= Deep Convolutional Neural Networks
Deep Convolutional Neural Networks (CNNs) are neural networks containing many convolutional layers, and they are commonly used in tasks such as image classification and object identification. The convolutional layers act as filters which can extract certain features from an image. A simple example of this is a convolution which identifies where in an image any horizontal lines are located. The exact filters are learned by the network during the training on a given dataset. One of the state-of-the-art CNNs is ResNet, which stands for Residual Network @ResNet. A ResNet is a type of CNN that avoids a common issue with deep neural networks called the vanishing gradient problem (VGP). The VGP is when the gradient, which is a measure of how the layers should change to lower the overall error of the model, becomes too small to cause a meaningful update to the earlier layers in a network. This is done by using a "skip connection" where the input to a block of layers is also added to it output. This gives the model an additional, shorter path to update the earlier layers, as shown in @resnetskipconnection. The issue with deep neural networks like ResNet is the resource requirements to run the models, with ResNet152 requiring 11.3 giga floating point operations per second (GFLOPS) even though not all inputs require the full model to run before the model has converged on an output.

#figure(
  image("./images/resnet-skip-connection.png"),
  caption: [
    An example on a skip connection, shown as Figure 2 in @ResNet
  ],
) <resnetskipconnection>

Neural Architecture Search is a method by which a neural network designs a new neural network for a given dataset.

Neural Architecture Search @NAS is a method through which a neural network is developed which can create architectures for other neural networks that should match the environment they will be deployed in better than  architectures which are manually designed. This paper describes the use of a recurrent neural network (RNN) to create the model descriptions for other neural networks. The RNN acts as a controllor which describes an architecture A with a probability p. This network is then trained to get an accuracy R. The controllor then computes the gradient of p and scales it by R, then reproduces the architecture A. This continues until a maximum R is reached. Through this method, each new architecture needs to be retrained which requires an incredibly large amount of computational power, likely beyond anything a single person could use. While this will likely never be efficient enough to run on the edge due to the large amount of training required, it is also important that the controllor RNN and architecture described for A should be compressed to use the least amount of computational resources powerful, as if this becomes the standard method of generating neural networks, the global power consumption required to generate neural networks could be massive. While this paper does prove this method has superior accuracy on the CIFAR-10 dataset, the ratio of accuracy to time and resources used to develop a network needs to stay at a maximum to ensure there isn't severe diminishing returns during their development.

These two papers show how large neural networks are achieving record accuracy and are being used to design novel architectures which could prove to be far more accurate for their trained environments. However, if left unchecked, these models could lead to inefficient deployments of models due to the resources necessary to run them. The wide scope of application of these models will become more and more useful in the next few years. For this reason it's important that these models can be compressed to a far more managable level in terms of storage and memory needed as well as the computational resources being lowered.

= Storing and Optimising Neural Networks
The Open Neural Network Exchange (ONNX) is both a file format and runtime environment for neural networks. It defines a standard set of operations that neural networks can perform, such as a Convolution or ReLU. All the operations that a certain model will perform is stored in a graph, with each node holding the relevant data for each operation, for example a convolutional will store all the weights an input will be multiplied by. This means all the needed data and all the operations that will be performed can be analyzed before the model is ever ran. This open framework is backed by most major techonolgy companies involved in machine learning and processor development, including AMD, Intel and Nvidia according to there website. All of these companies publically support the ONNX model and provide hardware acceleration for operations defined in the ONNX runtime. With this large amount of corporation support it would be beneficial for any optimisations to a model be performed in its ONNX representation as it will automatically have widespread hardware support.

Low-Level Virtual Machine (LLVM) is a compilation framework that can separate a high-level language from a low-level architecture @LLVM. They use an intermediate representation called LLVM IR which acts as a high-level assembly language. Frontends exist to compile a higher-level language into LLVM IR. For example, clang++ is a C++ compiler for LLVM IR, and rustc is the Rust compiler which compiles to LLVM IR. This IR can then be transformed and changed for optimisations. Then an architecture-specific backend can compile LLVM IR into binary for a given architecture. MLIR is a layer above an IR which allows the definition of a "dialect" where custom operations can be defined @mlir. This makes it easier to reason about optimisations and can then be compiled into LLVM IR, which can again be compiled into a binary.

ONNX-MLIR uses MLIR to compile a shared library from an ONNX file @onnxmlir. It has optimisation passes to allow for neural network graph transformation. Some optimisations are already built, but most are not run on ResNet50 and only work on smaller models. Optimisations for ONNX models can be implemented in the ONNX-MLIR passes to enable any optimisations on any backend architecture supported by LLVM. LLVM is a very mature framework, which means it has excellent support for optimising its' IR. Extending the ONNX-MLIR optimisation passes could see the speedups needed to enable deep CNNs on computationally restrictive hardware. Figure \ref{fig:onnxmlirpasses} shows the architecture used for ONNX-MLIR.

= Early Exiting
Early exiting in neural networks is an area that hasn't received much acemedic interest, with few papers being released on the topic in recent years. The key idea behind early exiting is that a network can already have identified the correct output without running through the entire model. An easy input can be correctly classified without the need for the entire depth of state-of-the-art models. To solve this, many exits may be placed somewhere along the model. At each of these exits, some condition is used as a measure of how confident the model is that its current classification is the correct one. For example, a CNN doing object identification may be able to correctly classify an image of a dog early in the network, but for an image of a complex machine, the network may need to use all layers to correctly classify the input. If a good condition is used to estimate the likelyhood of the current classification being correct, the average inference time can be greatly lowered. 

Some of the issues to consider with this approach are the time taken to decide on whether to exit (which may grow significant as the number of exits grows large and as an exits complexity increases), trying to analyse different exit architectures, specifically exits including or excluding pooling layers, and the amount of training time needed to search for the best placement for branches. However, the biggest problem for widespread use of early exiting is the need to change the architecture of a neural network to use exits. Currently, model architectures need to be manually changed to make use of exits which may need a significant amount of analysis after already training the main model. Ideally, whatever framework or compiler is used should automatically add exits in optimal locations for the model.

One of the most cited ways to add early exiting is the BranchyNet architecture, described in @BranchyNet. This architecture describes how exits should be structured when added to a model. In particular it describes how, for CNNs, an exit should consist of some number of convolution layers (the earlier the exit, the more convolutional layers) and a fully connected layer. Each branch can then be trained independently. The entropy of the softmax of the output of the early exit is used as the condition referred to as `e` in this report. That is, `e = entropy(softmax(y))`. This value is a measure of how confident the model is that it has accurately classified the input. If the model is unsure, many of the possible classes will have a high value in the output, resulting in a high entropy. If the model has narrowed in on only a few possible options for the output class, most of the classes in the output will have a tiny value resulting in a smaller entropy. Each exit also has an associated threshold entropy value. If `e` is less than the threshold `T`, the model presumes it is correct, and the softmax of the output is returned. That is, `if e < T then return softmax(y) else continue`. This architecture is the basis of most of the recent research into early exiting. However, @BranchyNet discusses neither the optimal location of branches or the optimal number of branches per model. Ideally, a method for adding early exits automatically to an architecture should be smart enough to find the optimal placement and number of branches to use for a given architecture. The threshold for the exit is also presented as a hyperparameter for each branch to be learned during training-time in the paper, but may be decided by other methods.

@optimalbranchplacement discusses an algorithm which, when given a set of different combinations of branches, will analyse which combination of branches reduces the average inference time the most. However, it is impractical to give a list of all possible combinations of exits to the algorithm, as an early exit could in theory be placed after every layer of a network, but that may be computationally expensive. Ideally, a method of automatically adding early exits shouldn't need to exhaustively check every possible combination, but should instead only look at combinations at least better than the current considered batch of early exits. @optimalbranchplacement also noted that, instead of having convolutional layers followed by a fully-connected layer, just having a fully-connected layer performs approximately equally to more complex exits.

@earlyexitmasters looks at different architectures that can be used as an exit, including single fully-connected layers, multiple fully-connected, and fully-connected layers combined with some pooling or batch normalisation layers. All exits shown in this paper have the confidence value of the model bounded between 0 and 1 by using a sigmoid function. Both the confidence value and the actual output from the network are outputted from the exit. A custom loss function is also presented that takes into account the cost of the model in terms of floating point operations (FLOPs), preferring lower cost exits, and also the accuracy and confidence of the model. Exits are also distributed using both self-similar methods and linear methods. This paper shows significant improves in performance without a significant decrease in accuracy. For example, an early exit ResNet152 trained on the SVHN database needs 2% of the relative cost of the original model, while losing 1% accuracy from 95.68% to 94.68%. Most models can keep the same accuracy with 20% of the relative cost. This paper shows how having an exit can have significant savings, but the exit has the confidence as a learnable trait, which differs from the solutions offered by this project.

- Mention how the training method using the final exit as a teacher and branches as students may allow a compile time solution to train a model after compilation

= Split Computing
Early exiting is also useful for moving towards edge computing. As neural networks uses more personal data, there is a greater need for privacy, and one of the best ways to do that is to have models running on edge devices, such as mobile phones and personal computers. Some architectures have been proposed to run a section of the model on an edge device, and if that section of the model is not able to exit confidently, then the rest of the inference is carried out in a central server. Early exiting from BranchyNet is used to assess how confident the edge device is about having an accurate output. One example of this is @TowardsEdgeComputing, which describes how to split the network to function across multiple devices.

- How can automatically adding exits benefit split computing

- Get papers from the review survey that support using the final exit as a teacher and explain why it is useful, as well as ones that train all branches individually
- Explain why training individually lets implementations be easy, so it is a good start for early exit implementation
